{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92aa573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f62b411",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\SaWa\\Brain Tumors\\Brain Tumors Data\\brain_tumor_mri\\new_dataset\\bt_images'\n",
    "\n",
    "# Construct full paths for the pickle files\n",
    "training_data_path = os.path.join(data_path, r'C:\\Users\\SaWa\\Brain Tumors\\Brain Tumors Data\\brain_tumor_mri\\new_dataset\\training_data.pickle')\n",
    "labels_path = os.path.join(data_path, r'C:\\Users\\SaWa\\Brain Tumors\\Brain Tumors Data\\brain_tumor_mri\\new_dataset\\labels.pickle')\n",
    "test_data_path = os.path.join(data_path, r'C:\\Users\\SaWa\\Brain Tumors\\Brain Tumors Data\\test_images-20210704T210303Z-001\\test_images')\n",
    "\n",
    "# Load the pickle files\n",
    "with open(training_data_path, 'rb') as training_data_file:\n",
    "    training_data = pickle.load(training_data_file)\n",
    "\n",
    "with open(labels_path, 'rb') as labels_file:\n",
    "    labels = pickle.load(labels_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ec9334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Width: 512\n",
      "Image Height: 512\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Open an image\n",
    "image_path = r'C:\\Users\\SaWa\\Brain Tumors\\Brain Tumors Data\\brain_tumor_mri\\new_dataset\\bt_images\\1.jpg'\n",
    "img = Image.open(image_path)\n",
    "\n",
    "# Get the dimensions (width and height) of the image\n",
    "width, height = img.size\n",
    "\n",
    "print(\"Image Width:\", width)\n",
    "print(\"Image Height:\", height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "283d479c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SaWa\\AppData\\Local\\Temp\\ipykernel_17380\\1410008418.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training_data = np.array(training_data)\n"
     ]
    }
   ],
   "source": [
    "# Convert to NumPy arrays and normalize.\n",
    "training_data = np.array(training_data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be5d46ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy arrays and normalize.\n",
    "training_data = np.array(training_data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2cc8f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3064, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f63a2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Label Values: [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "unique_labels = np.unique(labels)\n",
    "print(\"Unique Label Values:\", unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea472b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract 1 from each label to make them start from 0\n",
    "adjusted_labels = labels - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "182fe421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Label Values: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "unique_labels1 = np.unique(adjusted_labels)\n",
    "print(\"Unique Label Values:\", unique_labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4abd8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all files in the folder\n",
    "file_list = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f66432f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out only image files (e.g., JPG, PNG, etc.)\n",
    "image_files = [file for file in file_list if file.lower().endswith(('.jpg', '.png', '.jpeg'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa139905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store processed image data\n",
    "image_data = []\n",
    "\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(data_path, image_file)\n",
    "    img = Image.open(image_path)\n",
    "    \n",
    "    # Convert to RGB format if not already\n",
    "    img = img.convert('RGB')\n",
    "    \n",
    "    # Resize the image to the desired dimensions\n",
    "    img = img.resize((224, 224))\n",
    "    \n",
    "    # Convert to NumPy array\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Normalize pixel values and convert to NumPy array\n",
    "    img_array = np.array(img) / 255.0  \n",
    "    \n",
    "    # Append the processed image array to the list\n",
    "    image_data.append(img_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "186fefe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of processed image arrays to a NumPy array\n",
    "images_array = np.array(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a57d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(images_array, adjusted_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94fb6d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "del image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "360395ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical format\n",
    "num_classes = len(np.unique(adjusted_labels))\n",
    "train_labels_categorical = to_categorical(train_labels, num_classes=num_classes)\n",
    "test_labels_categorical = to_categorical(test_labels, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9df9dcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "77/77 [==============================] - 52s 634ms/step - loss: 2.9776 - accuracy: 0.4782 - val_loss: 1.0341 - val_accuracy: 0.5742\n",
      "Epoch 2/15\n",
      "77/77 [==============================] - 21s 272ms/step - loss: 1.0237 - accuracy: 0.4782 - val_loss: 0.9584 - val_accuracy: 0.5742\n",
      "Epoch 3/15\n",
      "77/77 [==============================] - 24s 313ms/step - loss: 1.0117 - accuracy: 0.4835 - val_loss: 0.9360 - val_accuracy: 0.4796\n",
      "Epoch 4/15\n",
      "77/77 [==============================] - 22s 280ms/step - loss: 0.9916 - accuracy: 0.4790 - val_loss: 0.9614 - val_accuracy: 0.5041\n",
      "Epoch 5/15\n",
      "77/77 [==============================] - 22s 287ms/step - loss: 0.9859 - accuracy: 0.4843 - val_loss: 0.8970 - val_accuracy: 0.5546\n",
      "Epoch 6/15\n",
      "77/77 [==============================] - 21s 278ms/step - loss: 0.9616 - accuracy: 0.4986 - val_loss: 0.8874 - val_accuracy: 0.4551\n",
      "Epoch 7/15\n",
      "77/77 [==============================] - 21s 276ms/step - loss: 0.9488 - accuracy: 0.5014 - val_loss: 0.8566 - val_accuracy: 0.4698\n",
      "Epoch 8/15\n",
      "77/77 [==============================] - 22s 286ms/step - loss: 0.9339 - accuracy: 0.5088 - val_loss: 0.9002 - val_accuracy: 0.4666\n",
      "Epoch 9/15\n",
      "77/77 [==============================] - 22s 283ms/step - loss: 0.9344 - accuracy: 0.5108 - val_loss: 0.9136 - val_accuracy: 0.4470\n",
      "Epoch 10/15\n",
      "77/77 [==============================] - 23s 296ms/step - loss: 0.9410 - accuracy: 0.5067 - val_loss: 0.8370 - val_accuracy: 0.5710\n",
      "Epoch 11/15\n",
      "77/77 [==============================] - 23s 297ms/step - loss: 0.9278 - accuracy: 0.5165 - val_loss: 0.7910 - val_accuracy: 0.5856\n",
      "Epoch 12/15\n",
      "77/77 [==============================] - 22s 292ms/step - loss: 0.9421 - accuracy: 0.5165 - val_loss: 0.7994 - val_accuracy: 0.5726\n",
      "Epoch 13/15\n",
      "77/77 [==============================] - 21s 277ms/step - loss: 0.9075 - accuracy: 0.5328 - val_loss: 0.7854 - val_accuracy: 0.5824\n",
      "Epoch 14/15\n",
      "77/77 [==============================] - 22s 283ms/step - loss: 0.9059 - accuracy: 0.5337 - val_loss: 0.8116 - val_accuracy: 0.4747\n",
      "Epoch 15/15\n",
      "77/77 [==============================] - 21s 277ms/step - loss: 0.9187 - accuracy: 0.5312 - val_loss: 0.7979 - val_accuracy: 0.5106\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16bffc96ca0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_width = 224\n",
    "image_height = 224\n",
    "num_channels = 3  # Assuming RGB images\n",
    "num_classes = 3  # Number of classes in your dataset\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(image_width, image_height, num_channels)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=15, validation_data=(test_images, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08a12fc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 1s 34ms/step - loss: 0.7979 - accuracy: 0.5106\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluation = model.evaluate(test_images, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
